{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee73a0b2-3e97-413d-ae8b-f2aebfddf5fa",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "Arthur Olivieri, David Conselvan and Pedro Stanzani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bca635-718e-42a5-a764-b4698645f204",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "In this project, we aim to predict the Severity Impairment Index (sii), a measure of problematic internet use among adolescents, using data from the Healthy Brain Network (HBN) study. \n",
    "\n",
    "The dataset combines multi-modal information, including tabular data on:\n",
    "- demographics;\n",
    "- physical activity;\n",
    "- fitness;\n",
    "- sleep patterns; and\n",
    "- internet usage\n",
    "\n",
    "Our approach will involve thorough data preprocessing to handle missing values, normalize features, and extract meaningful summaries from the time-series data. We will perform exploratory data analysis (EDA) to understand the relationships between predictors and the target variable, followed by feature engineering to combine tabular and time-series data effectively. Using these features, we will build and evaluate machine learning models, such as ensemble models for tabular data and sequence models (e.g., RNNs or Transformers) for the accelerometer data. \n",
    "\n",
    "The integration of these models will enable us to develop a robust system for predicting problematic internet use, contributing insights to mental health research and potentially informing interventions for at-risk adolescents.\n",
    "\n",
    "View also: https://www.kaggle.com/c/child-mind-institute-problematic-internet-use/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adada990-1abc-4f67-b4a3-41056d3328c3",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea684d-07fb-4013-808e-7cd4838c3c36",
   "metadata": {},
   "source": [
    "Note in particular the field PCIAT-PCIAT_Total. The target sii for this competition is derived from this field as described in the data dictionary: 0 for None, 1 for Mild, 2 for Moderate, and 3 for Severe. Additionally, each participant has been assigned a unique identifier id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f860e2-bf83-4521-8acb-14d89fffd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def PCIAT_PCIAT_Total_to_sii(PCIAT_PCIAT_Total):\n",
    "    if np.isnan(PCIAT_PCIAT_Total):\n",
    "        return np.nan\n",
    "    elif 0 <= PCIAT_PCIAT_Total <= 30:\n",
    "        return 0  # None\n",
    "    elif 31 <= PCIAT_PCIAT_Total <= 49:\n",
    "        return 1  # Mild\n",
    "    elif 50 <= PCIAT_PCIAT_Total <= 79:\n",
    "        return 2  # Moderate\n",
    "    elif 80 <= PCIAT_PCIAT_Total <= 100:\n",
    "        return 3  # Severe\n",
    "    else:\n",
    "        raise ValueError(\"Invalid PCIAT_PCIAT_Total value. It must be between 0 and 100 or NaN.\")\n",
    "\n",
    "def get_severity_level(score):\n",
    "    if 0 <= score <= 30:\n",
    "        return 'None'\n",
    "    elif 31 <= score <= 49:\n",
    "        return 'Mild'\n",
    "    elif 50 <= score <= 79:\n",
    "        return 'Moderate'\n",
    "    elif 80 <= score <= 100:\n",
    "        return 'Severe'\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a88a49-d9a8-46d0-8ace-11d0825fdfcb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94df0fb-2476-42f2-a057-e6605f4f60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c84dd6-536a-4ed4-89a6-d61af50c0668",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1f79e-0978-4d6f-b58d-18799fa6a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "sns.set_palette(\"muted\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253791a-db8c-428e-941b-290b111a1d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "data_dict = pd.read_csv('../data/data_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b53ed1-8fc6-41be-b639-70460b09adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(subset=['sii'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2f725-e4f9-4ce2-917d-af789cd22870",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_related_columns = [col for col in train_data.columns if col not in test_data.columns]\n",
    "target_related_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d8eae-0d00-4a50-83b2-674543fb57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd63e2-f9b1-402a-9077-a0af8691f321",
   "metadata": {},
   "source": [
    "### 1.1. Column information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495fdd4-aaae-4249-80d9-a4fc7f07415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6cd29-2f4e-491e-9dd6-d94ab87b8866",
   "metadata": {},
   "source": [
    "### 1.2. Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7aef2-d3fd-4d13-99c9-8760b907842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train_data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(train_data)) * 100\n",
    "\n",
    "print(missing_percentage[missing_percentage > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee8051-2003-4d58-a396-b82e43ed29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(train_data.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f31d1d-0839-4667-b79a-c1ec1adc1dcc",
   "metadata": {},
   "source": [
    "### 1.3.  Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160c511-6d74-482e-a817-de835b385732",
   "metadata": {},
   "source": [
    "**Descriptive statistics (numerical data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ae6bc-d9a6-4f96-842f-d67370dbcc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3b700-0991-4268-9bf8-35699366e72e",
   "metadata": {},
   "source": [
    "**Descriptive statistics (Categorical Data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68fe23-cc3b-4a8f-a033-e0f7ccd969c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe(include=['object', 'category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f27838-e831-4cf9-a362-be714c18aa02",
   "metadata": {},
   "source": [
    "### 1.4. Analyze the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3de369-8bcc-4990-9817-38210419c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Severity_Level'] = train_data['PCIAT-PCIAT_Total'].apply(get_severity_level)\n",
    "train_data['Severity_Level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be340cc2-c7af-4f95-8538-04fbb71cf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of Severity Levels\n",
    "sns.countplot(x='Severity_Level', data=train_data, order=['None', 'Mild', 'Moderate', 'Severe'])\n",
    "plt.title('Distribution of Severity Levels')\n",
    "plt.xlabel('Severity Level')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of PCIAT_Total scores\n",
    "sns.histplot(train_data['PCIAT-PCIAT_Total'], bins=30, kde=True)\n",
    "plt.title('Distribution of PCIAT_Total Scores')\n",
    "plt.xlabel('PCIAT_Total Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2ef86-2a22-499d-b98f-f4ca36b4fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class imbalance\n",
    "severity_counts = train_data['Severity_Level'].value_counts()\n",
    "severity_counts / len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005501e-f5c4-4556-a37a-fe75d23579f7",
   "metadata": {},
   "source": [
    "### 1.5. Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89351d53-b107-4dbe-99b2-a324bc1e4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_features = [\n",
    "  \"Basic_Demos-Age\",\n",
    "  \"CGAS-CGAS_Score\",\n",
    "  \"Physical-BMI\",\n",
    "  \"Physical-Height\",\n",
    "  \"Physical-Weight\",\n",
    "  \"Physical-Waist_Circumference\",\n",
    "  \"Physical-Diastolic_BP\",\n",
    "  \"Physical-HeartRate\",\n",
    "  \"Physical-Systolic_BP\",\n",
    "  \"Fitness_Endurance-Time_Mins\",\n",
    "  \"Fitness_Endurance-Time_Sec\",\n",
    "  \"FGC-FGC_CU\",\n",
    "  \"FGC-FGC_GSND\",\n",
    "  \"FGC-FGC_GSD\",\n",
    "  \"FGC-FGC_PU\",\n",
    "  \"FGC-FGC_SRL\",\n",
    "  \"FGC-FGC_SRR\",\n",
    "  \"FGC-FGC_TL\",\n",
    "  \"BIA-BIA_BMC\",\n",
    "  \"BIA-BIA_BMI\",\n",
    "  \"BIA-BIA_BMR\",\n",
    "  \"BIA-BIA_DEE\",\n",
    "  \"BIA-BIA_ECW\",\n",
    "  \"BIA-BIA_FFM\",\n",
    "  \"BIA-BIA_FFMI\",\n",
    "  \"BIA-BIA_FMI\",\n",
    "  \"BIA-BIA_Fat\",\n",
    "  \"BIA-BIA_ICW\",\n",
    "  \"BIA-BIA_LDM\",\n",
    "  \"BIA-BIA_LST\",\n",
    "  \"BIA-BIA_SMM\",\n",
    "  \"BIA-BIA_TBW\",\n",
    "  \"PAQ_A-PAQ_A_Total\",\n",
    "  \"PAQ_C-PAQ_C_Total\",\n",
    "  # \"PCIAT-PCIAT_Total\",\n",
    "  \"SDS-SDS_Total_Raw\",\n",
    "  \"SDS-SDS_Total_T\",\n",
    "]\n",
    "\n",
    "# Visualize distributions using histograms\n",
    "train_data[numerical_features].hist(bins=15, figsize=(15, 20), layout=(len(numerical_features) // 3 + 1, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze correlations between numerical features and the target variable\n",
    "corr_features = numerical_features + ['PCIAT-PCIAT_Total']\n",
    "corr_data = train_data[corr_features]\n",
    "correlation_matrix = corr_data.corr()\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ba950-9fda-4d5e-b4b2-d1d362af5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix['PCIAT-PCIAT_Total'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf923a-55fa-4dd4-9b3b-584859e03ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that may not be in the dataset\n",
    "categorical_features = [\n",
    "    \"Basic_Demos-Enroll_Season\",\n",
    "    \"CGAS-Season\",\n",
    "    \"Physical-Season\",\n",
    "    \"Fitness_Endurance-Season\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"FGC-Season\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "    \"FGC-FGC_GSND_Zone\",\n",
    "    \"FGC-FGC_GSD_Zone\",\n",
    "    \"FGC-FGC_PU_Zone\",\n",
    "    \"FGC-FGC_SRL_Zone\",\n",
    "    \"FGC-FGC_SRR_Zone\",\n",
    "    \"FGC-FGC_TL_Zone\",\n",
    "    \"BIA-Season\",\n",
    "    \"BIA-BIA_Activity_Level_num\",\n",
    "    \"BIA-BIA_Frame_num\",\n",
    "    \"PAQ_A-Season\",\n",
    "    \"PAQ_C-Season\",\n",
    "    \"SDS-Season\",\n",
    "    \"PreInt_EduHx-Season\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"Basic_Demos-Sex\"\n",
    "]\n",
    "\n",
    "categorical_features = [feature for feature in categorical_features if feature in train_data.columns]\n",
    "\n",
    "# Analyze the distribution of categories for key features\n",
    "fig, axes = plt.subplots(nrows=(len(categorical_features) + 2) // 3, ncols=3, figsize=(18, 5 * ((len(categorical_features) + 2) // 3)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    sns.countplot(x=feature, data=train_data, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Distribution of {feature}')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(len(categorical_features), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize relationships between categorical features and the target variable\n",
    "fig, axes = plt.subplots(nrows=(len(categorical_features) + 2) // 3, ncols=3, figsize=(18, 5 * ((len(categorical_features) + 2) // 3)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    sns.countplot(x=feature, hue='Severity_Level', data=train_data, order=train_data[feature].value_counts().index, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} vs Severity Level')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].legend(title='Severity Level')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(len(categorical_features), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de54636e-84d4-4202-8b8e-32b69d004652",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a29dde-612b-4426-96fc-07e6f3f76eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_column = 'sii'\n",
    "X = train_data[numerical_features + categorical_features]\n",
    "y = train_data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    \n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),  # Standardize numerical features\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # One-hot encode categorical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the transformer on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform both train and test data\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Output shapes\n",
    "print(f\"Transformed X_train shape: {X_train_transformed.shape}\")\n",
    "print(f\"Transformed X_test shape: {X_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b17e30-8672-46e6-8ad1-b2b5d7266566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Handle NaN values\n",
    "    ('dim_reduction', PCA()),\n",
    "    ('classifier', RandomForestClassifier())  # Placeholder\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc69ac5-2a70-4536-874b-f2764a8fdb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Models to use\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVC': SVC(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'Dummy': DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "}\n",
    "\n",
    "# Example: Setting the pipeline's classifier to one of these models\n",
    "pipeline.set_params(classifier=models['LogisticRegression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543af66-8fd2-4b4f-91fa-da3c7ee02c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = [\n",
    "    {\n",
    "        'dim_reduction__n_components': [5, 10, 20, 50],\n",
    "        'classifier': [SVC()],\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction__n_components': [5, 10, 20, 50],\n",
    "        'classifier': [LogisticRegression(max_iter=1000, random_state=42)],\n",
    "        'classifier__C': [0.1, 1, 10]\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction__n_components': [5, 10, 20, 50],\n",
    "        'classifier': [RandomForestClassifier(random_state=42)],\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20]\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction__n_components': [5, 10, 20, 50],\n",
    "        'classifier': [GaussianNB()]\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "# Use GridSearchCV with combined parameter grids\n",
    "grid_search = GridSearchCV(pipeline, param_grids, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce628712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output best parameters and score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470bedec",
   "metadata": {},
   "source": [
    "### 3. Re-train best models and Deploy the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e480dff-5f56-4912-b0d1-ce62e60ccccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "target_names = ['None', 'Mild', 'Moderate', 'Severe']\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Balancing the classes\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "best_model.named_steps['classifier'].set_params(class_weight=dict(enumerate(class_weights)))\n",
    "\n",
    "# Retrain the best model on the training dataset\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report for the Best Model:\")\n",
    "print(classification_report(y_test, y_pred, target_names= target_names))\n",
    "\n",
    "# Evaluate the dummy classifier\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_dummy_pred = dummy_clf.predict(X_test)\n",
    "print(\"Classification Report for the Dummy Classifier:\")\n",
    "print(classification_report(y_test, y_dummy_pred, target_names= target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a92e4",
   "metadata": {},
   "source": [
    "### 4. Evaluation of Results: Addressing Class Imbalance and Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "balanced_acc_dummy = balanced_accuracy_score(y_test, y_dummy_pred)\n",
    "\n",
    "print(\"Normal accuracy Best Model\",accuracy_score(y_test, y_pred))\n",
    "print(\"Normal accuracy Dummy\", accuracy_score(y_test, y_dummy_pred))\n",
    "print(\"Balanced Accuracy Best Model:\", balanced_acc)\n",
    "print(\"Balanced Accuracy Dummy:\", balanced_acc_dummy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3f9d2",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "The dataset used in this analysis exhibits significant class imbalance. The majority class (None) accounts for ~58% of the data, while the Severe class represents only ~1%. This imbalance poses a challenge for the classifier, as models tend to favor predicting the majority class, leading to misleadingly high accuracy but poor performance on minority classes.\n",
    "\n",
    "##### Metrics Analysis\n",
    "\n",
    "\n",
    "- Normal Accuracy:\n",
    "\n",
    "    - Best Model: 52.92% \n",
    "\n",
    "\n",
    "    - Dummy Classifier: 61.31%\n",
    "\n",
    "\n",
    "The dummy classifier achieves higher accuracy by always predicting the majority class (None). However, this is not a meaningful success as it completely ignores minority classes.\n",
    "\n",
    "\n",
    "The best model, while less accurate overall, attempts to balance predictions across classes, leading to lower accuracy but better generalization to minority classes.\n",
    "Balanced Accuracy:\n",
    "\n",
    "- Best Model: 53.64%\n",
    "\n",
    "\n",
    "- Dummy Classifier: 25.00%\n",
    "\n",
    "\n",
    "Balanced accuracy measures the average recall across all classes, giving equal importance to each. The best model significantly outperforms the dummy classifier here, showing its ability to make predictions across all classes, even minority ones.\n",
    "\n",
    "\n",
    "The dummy classifierâ€™s poor balanced accuracy reflects its complete failure to predict any minority classes (Mild, Moderate, Severe)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05241e1d",
   "metadata": {},
   "source": [
    "### 5. Predicting with the competition's test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = test_data[numerical_features + categorical_features]\n",
    "y_final = best_model.predict(X_final)\n",
    "y_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af27975",
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in test_data.columns if 'id' in col.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ''\n",
    "for i, row in test_data.iterrows():\n",
    "    s += f'{row['id']},{int(y_final[i])}\\n'\n",
    "\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,sii\\n')\n",
    "    f.write(s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
